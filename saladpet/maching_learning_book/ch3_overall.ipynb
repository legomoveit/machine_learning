{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # 머신러닝의 정의\r\n",
    "# - 어떤 작업 T를 수행했을때 경험 E로 인해 성능이 P만큼 향상되었다면 이 컴퓨터 프로그램은 작업 T와 성능 P에 대해 경험 E를 학습한것이다\r\n",
    "\r\n",
    "# # 머신러닝의 분류\r\n",
    "# # 1. 지도학습모형\r\n",
    "# 정답 있는것\r\n",
    "# 분류모형과 수치예측모형으로 구분할 수 있다\r\n",
    "# 선형회귀모형, 로지스틱 회귀분석, K-최근접이웃, 의사결정나무, 서포트 벡터 머신, 나이브 베이즈, 인공신경망, 딥러닝, 앙상블 기법\r\n",
    "\r\n",
    "# # 2. 비지도학습 모형\r\n",
    "# 종속변수가 존재하지 않음\r\n",
    "# 군집분석, 연관규칙분석, 협업필터링, 텍스트 마이닝"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "#머신러닝의 과정\r\n",
    "\r\n",
    "# A. 데이터 처리\r\n",
    "# 1. 데이터 준비\r\n",
    "# 2. 데이터 변환 : 원천데이터의 표현방식이나 척도를 분석목적과 기법에 적합하도록 변환\r\n",
    "# 3. 파생변수의 생성 : 기존 필드(변수)들을 조합, 가공하여 새로운 변수를 파생시킴\r\n",
    "# 4. 결측치 / 오류값의 처리 : 분석에 사용될 주요 필드에 대한 결측치나 오류값이 발견될 경우 추정하여 새로운 값으로 채우거나 변환절차를 거쳐 데이터를 수정\r\n",
    "\r\n",
    "# 데이터 분할 : 학습용 데이터 세트(학습용 + 검증용) + 평가용 데이터 세트\r\n",
    "# 학습 / 검증 / 평가 데이터 세트의 비율을 대개 70:20:10 으로 설정함\r\n",
    "\r\n",
    "# B. 모형학습\r\n",
    "# 1. 분석모형의 결정 : 해결하고자 하는 비즈니스 목적에 따라 적합하다고 판단되는 복수개의 머신러닝 모형 선정\r\n",
    "# 2. 인자의 지정\r\n",
    "# 3. 학습 및 인자 조절 : 학습된 모형의 결과를 파악하면서 모형의 성능을 더 좋게 개선하기 위해 인자값을 조정해 나간다\r\n",
    "# 주의해야 할 부분 : 모형이 과잉적합(모형이 데이터에 너무나 학습이 잘 되어 복잡해짐)이나 과소적합(데이터를 충분히 학습하지 않아 너무 단순하 모형으로 귀결)되지 않도록\r\n",
    "\r\n",
    "# C. 성능평가"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# # 모형 평가\r\n",
    "\r\n",
    "# # A. 성능평가지표---------------------------------------------------------------------\r\n",
    "# 분류모형을 위한 평가지표와 수치예측모형을 위한 평가지표로 구분하여 사용해야 한다\r\n",
    "\r\n",
    "# #1. 분류모형 평가지표\r\n",
    "# 배경지식 : \r\n",
    "# 예측결과 양성 / 실제결과 양성 : 참양성\r\n",
    "# 예측결과 양성 / 실제결과 음성 : 거짓 양성\r\n",
    "# 예측결과 음성 / 실제결과 양성 : 거짓 음성\r\n",
    "# 예측결과 음성 / 실제결과 음성 : 참음성\r\n",
    "\r\n",
    "# - 정확도\r\n",
    "# 참양성 + 참음성 / 참양성 + 참음성 + 거짓양성 + 거짓음성\r\n",
    "# 최소 50 % 이상이어야 함\r\n",
    "\r\n",
    "# - 정밀도 : 예측결과가 양성인것들 중 실제로 양성인 얘들\r\n",
    "# 참양성 / 참양성 + 거짓양성\r\n",
    "\r\n",
    "# - 재현율 : 실제로 양성인것들 중 예측결과가 양성으로 나온 비율\r\n",
    "# 참양성 / 참양성 + 거짓음성\r\n",
    "\r\n",
    "# - F1 스코어\r\n",
    "# 2 x ((정밀도 x 재현율) / (정밀도 + 재현율))\r\n",
    "\r\n",
    "# - ROC 곡선\r\n",
    "# 기준선보다 위쪽으로 볼록할수록 유의한것임\r\n",
    "# AUC : area under the curve 가 0.5 이상이어야 하고 1에 가까울수록 우수한 성능\r\n",
    "\r\n",
    "# #2. 수치예측(회귀)모형 평가지표\r\n",
    "# - 평균제곱오차 (MSE)\r\n",
    "# 오차의 제곱에 평균을 취한 값\r\n",
    "\r\n",
    "# - 평균 제곱근 오차 (RMSE)\r\n",
    "# 평균제곱오차에 제곱근(루트) 씌운것\r\n",
    "\r\n",
    "# - 결정계수\r\n",
    "# 수치의 측정단위와 상관없이 0에서 1사이의 값을 가지는 수치\r\n",
    "\r\n",
    "# # B. 교차검증-------------------------------------------------------------------------\r\n",
    "\r\n",
    "# # 1. K-겹 교차검증 (수치예측) >> 아래 예시 있음\r\n",
    "# 전체데이터를 K등분하여 K번 모델링을 통해 모형 평가\r\n",
    "# K = 3 이면 첫번째 모형은 1번째 데이터를 평가용, 나머지를 학습용\r\n",
    "# 두번째 모형은 2번째 데이터를 평가용, 나머지를 학습용\r\n",
    "\r\n",
    "# # 2. 층화 K-겹 교차검증 (분류) >> 아래 예시 있음\r\n",
    "# K겹교차검증으로 분류예측하면 특정 폴드(K개로 나눈 그 각각의 데이터들)내에서 분류할 클래스(범주)의 비율이 전체대상의 비율과는 다르게 샘플림 될 위험이 있음\r\n",
    "# K겹교차검증 기능에 층화표본추출 개념을 추가한 방법임\r\n",
    "# 각 폴드 내에서의 클래스 비율이 전체 데이터세트의 클래스 비율과 같도록 데이터를 나누어 K겹교차검증을 진행하는거임\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "##### K겹 교차검증 예시\r\n",
    "import pandas as pd\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.neighbors import KNeighborsRegressor\r\n",
    "from sklearn.model_selection import crass_val_score\r\n",
    "\r\n",
    "### 데이터 불러오기\r\n",
    "df = ...\r\n",
    "\r\n",
    "### 변수지정 (독립변수 / 종속변수)\r\n",
    "X = df[열1,열2,열3]\r\n",
    "Y = df[열4]\r\n",
    "\r\n",
    "### 데이터 분할 (학습용/평가용)\r\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.3, random_state = 0)\r\n",
    "#test size는 평가용 데이터셋 비율, random_state 는 난수초기값 정하는건데 아무거로나 하면 됨\r\n",
    "\r\n",
    "### 모형생성\r\n",
    "model = KNeighborsRegressor()\r\n",
    "\r\n",
    "### 5-겹 교차검증 수행\r\n",
    "scores = cross_val_score(model, X_train, Y_train, cv = 5)\r\n",
    "# cv 가 겹 수 정하는거임\r\n",
    "print(scores)\r\n",
    "#결과로 5개의 값 나오는데 결정계수의 수준이 그 범위인거임\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "##### 층화 K-겹 교차검증 예시\r\n",
    "from sklearn.neighbors import KNeighborsClassifier\r\n",
    "from sklearn.model_selection import StratifiedKFold\r\n",
    "\r\n",
    "### 변수지정 (독립변수 / 종속변수)\r\n",
    "X = df[열1,열2,열3]\r\n",
    "Y = df[열4]\r\n",
    "\r\n",
    "### 데이터 분할 (학습용/평가용)\r\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.3, random_state = 0)\r\n",
    "#test size는 평가용 데이터셋 비율, random_state 는 난수초기값 정하는건데 아무거로나 하면 됨\r\n",
    "\r\n",
    "### 모형생성\r\n",
    "model = KNeighborsRegressor()\r\n",
    "\r\n",
    "## 층화 5-겹 교차검증 수행\r\n",
    "skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 0)\r\n",
    "# shuffle = 폴드를 나누기 전에 각 클래스의 데이터를 섞을 지에대한 여부\r\n",
    "scores = cross_val_score (model, X_train, Y_train, cv = skf)\r\n",
    "print(scores)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### 성능향상\r\n",
    "\r\n",
    "## A. 변수선택 ---------------------------------------------------------------\r\n",
    "\r\n",
    "# 1. 일변량 통계기반 선택\r\n",
    "# 분류문제 >> 카이스퀘어 값\r\n",
    "# 수치예측문제 >> 피어슨상관계수 값\r\n",
    "\r\n",
    "# p값에 기초하여 높은 p값을 갖는 변수들을 순차적으로 제거\r\n",
    "SelectKBest() >> #아래 예시# 혹은 SelectPrecentile() 함수를 주로 사용\r\n",
    "\r\n",
    "## B. 모형기반선택 ---------------------------------------------------------------\r\n",
    "# 지도학습 머신러닝 모형을 이용하여 변수의 중요도 평가\r\n",
    "# 변수선택을 위한 모형은 각 변수의 중요도를 측정할 수 있어야 한다\r\n",
    "# 가령 의사결정나무모형은 변수의 중요도가 담겨있는 feature_importances_ 속성을 제공하므로 가능 >> #예시#\r\n",
    "# 일변량통계기반 손택에서는 변수들간의 상호작용 고려 x, but 이거는 반영 가능\r\n",
    "\r\n",
    "## C. 데이터 밸런싱--------------------------------------------------------\r\n",
    "# 정확도가 데이터의 양에 의해 결정되면 안되기에 데이터의 클래스(범주) 비율 같게 만들어줘야함\r\n",
    "# 언더샘플링 : 다수 클래스의 값들 제거 #아래예시#\r\n",
    "# 오버샘플링 : 소수 클래스 값들 증가 #아래예시#\r\n",
    "# SMOTE > 오버샘플링의 대표적 방법 : 적은 데이터세트에 있는 개별 데이터들의 K최근접이웃을 찾고 개별데이터와 K개이웃들의 차이를 일정겂으로 만들어서 기준 데이터와 약간 차이난느 새로운 데이터 만듬\r\n",
    "# 보통 오버샘플링함\r\n",
    "# pip install imbalanced-learn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\r\n",
    "\r\n",
    "### 변수지정 (독립변수/종속변수)\r\n",
    "X = df.drop(['고객ID','이탈여부'],axis=1)\r\n",
    "#안쓰는 열 뺀거임\r\n",
    "Y = df['이탈여부']\r\n",
    "\r\n",
    "### 데이터 분할 (학습용/평가용 데이터 세트)\r\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.3, random_state = 0)\r\n",
    "#test size는 평가용 데이터셋 비율, random_state 는 난수초기값 정하는건데 아무거로나 하면 됨\r\n",
    "\r\n",
    "### 일변량 통계기반 변수 선택\r\n",
    "feat_selector = SelectKBest(chi2)\r\n",
    "feat_selector.fit(X_train, Y_train)\r\n",
    "\r\n",
    "### 선택된 변수 출력\r\n",
    "feat_scores = pd.DataFrame()\r\n",
    "feat_scores[\"Chi_sqaured stats\"] = feat_selector.scores_\r\n",
    "feat_scores[\"P Value\"] = feat_selector.pvalues_\r\n",
    "feat_scores[\"Support\"] = feat_selector.get_support()\r\n",
    "feat_scores[\"Attribute\"] = X_train.columns\r\n",
    "feat_scores[feat_scores[\"Support\"]==True]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "##### 모형기반 변수선택 (의사결정나무 모형 이용) 예시\r\n",
    "\r\n",
    "from sklearn.feature_selection import RFE\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "### 반복적 변수 선택\r\n",
    "select = RFE(RandomForestClassifier(n_estimators=100, random_state = 0), n_features_to_select = 5)\r\n",
    "# 최종선택할 변수의 개수는 n_features_to_select인자를 통해 설정 가능, 기본값은 입력된 독립변수 개수의 반\r\n",
    "select.fit(X_train, Y_train)\r\n",
    "\r\n",
    "### 선택된 변수 출력\r\n",
    "features_bool = np.array(select.get_support())\r\n",
    "features = np.array(X.columns)\r\n",
    "result = features[features_bool]\r\n",
    "print(result)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "####  랜덤언더샘플링 예시\r\n",
    "\r\n",
    "from imblearn.under_sampling import RandomUnderSampler\r\n",
    "from collections import Counter\r\n",
    "\r\n",
    "## 랜덤 언더 샘플링 실행\r\n",
    "X_train_under, Y_train_under = \\\r\n",
    "    RandomUnderSampler(random_state = 0).fit_sample(X_train, Y_train)\r\n",
    "\r\n",
    "## 결과 출력\r\n",
    "print('Original dataset shape %s'% Counter(Y))\r\n",
    "#counter가 개수 세주는거임\r\n",
    "print('Original dataset shape %s'% Counter(Y_train))\r\n",
    "print('Original dataset shape %s'% Counter(Y_train_under))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "##### SMOTE 예시\r\n",
    "from imblearn.over_sampling import SMOTE\r\n",
    "\r\n",
    "# SMOTE 샘플링 실행\r\n",
    "X_train_over, Y_train_over = SMOTE(random_state = 0).fit_sample(X_train, Y_train)\r\n",
    "\r\n",
    "# 결과 출력\r\n",
    "pprint('Original dataset shape %s'% Counter(Y))\r\n",
    "#counter가 개수 세주는거임\r\n",
    "print('Original dataset shape %s'% Counter(Y_train))\r\n",
    "print('Original dataset shape %s'% Counter(Y_train_under))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "##### 그리드서치\r\n",
    "\r\n",
    "# 대부분의 머신러닝 모형은 고유 인자가 존재함. 최적의 인자값을 결정하기 위한 방법\r\n",
    "# 가능한 모든 인자값의 조합을 시도하여 가장 우수한 성능을 보이는 값을 찾아줌\r\n",
    "# 아래 예시"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## 그리드서치 예시 (K-NN 분류모형으로 진행)\r\n",
    "from sklearn.model_selection import GridSearchCV\r\n",
    "\r\n",
    "## 1. 변수지정 (독립변수 / 종속변수)\r\n",
    "X = df.['열1', '열2']\r\n",
    "Y = df['열3']\r\n",
    "\r\n",
    "## 2. 데이터분할 (학습용/평가용 데이터 세트)\r\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size = 0.3, random_state = 0)\r\n",
    "\r\n",
    "## 3. 검색대상인자생성\r\n",
    "mylist = list(range(1,50))\r\n",
    "k_list = [x for x in mylist if x %2 !=0] #1~50 중 2로나눈게 0이 아닌 == 홀수인 것들\r\n",
    "# K-NN 모형의 K 는 클래스(범주)를 판단하기 위해 참고하는 최근접이웃의 개수로 과반수로 클래스 판단해서 보통 홀수로 설정\r\n",
    "parameter_grid = {'n_neighbors':k_list}\r\n",
    "\r\n",
    "## 4. 그리드서치 진행\r\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), parameter_grid, cv = 10)\r\n",
    "grid_search.fit(X_train, Y_train)\r\n",
    "print('최적의 인자 : ', grid_search.best_params_)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}